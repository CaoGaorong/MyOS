[toc]

## 操作系统：内存管理

###  一、什么是内存管理

操作系统中的内存空间是有限的，比如这里我要实现的操作系统是32位的，地址总线是32位，可管理物理内存是2<sup>32</sup>位，也就是4GB。

而操作系统的一大作用，就是管理计算机的资源，其中就包括管理内存空间。那么操作系统如何管理内存空间呢？

说白了，管理内存，就是要对内存增删改查。

#### 1. 虚拟地址和物理内存

在我这个实现的操作系统中，是使用了分页，开启了页表的，也就是实现了虚拟内存机制，因此，管理内存就有两部分：

- 管理物理内存  -> 物理内存池
- 管理虚拟地址  -> 虚拟地址池

> 物理内存，就是计算机中实际的内存空间。
>
> 虚拟地址，是每个进程可以分配的虚拟内存空间，最终会通过页表，映射到物理内存中。虚拟地址本身也要占用物理内存。

> 这里为了更好地区分**物理内存**和**虚拟内存**，对于**虚拟内存**，我称为 <font color="red">虚拟地址</font>，因为虚拟内存，本质上就是虚拟出来地址，让每个进程有4G(对于32位操作系统而言)的可分配地址（虚拟的）。

>  <font color="red">注意：这里一定要分清楚虚拟内存和物理内存，不然在分配内存是很容易晕。要时刻注意当前是在操作物理内存还是虚拟地址</font>。



#### 2. 内核物理内存和用户物理内存

对于操作系统和用户进程对内存的使用，我们应该要区分开来，不然用户进程把物理内存空间都占用了，操作系统都无法运行。

所以必须把物理内存分为两个部分：

- 内核物理内存  -> 内核物理内存池
- 用户物理内存 -> 用户物理内存池

> <font color="red">注意，我这里强调，是物理内存，把<b>物理</b>内存分为内核物理内存和用户物理内存。因为物理内存是所有用户进程，和操作系统共用的。</font>



所以对内存管理如下图，可以分为两个维度：

- 内核空间和用户空间
- 虚拟地址和物理内存

![](https://gitee.com/imcgr/image_blog/raw/master/20210614085847.png)



- 对于内核空间，所有内核线程只有一个内核虚拟地址池，一个内核物理内存池
- 对于用户空间，每个用户进程都有一个虚拟地址池（每个进程有一个页表），所有用户进程共用1个用户物理内存池



#### 3. 如何管理虚拟地址和物理内存

管理内存，肯定要用一个数据结构来管理，这里不管是管理物理内存还是虚拟地址，都使用的是**位图**。

位图，用一**位**来表示一个物理页的内存的使用情况。位图中，该位为1，表示对应的物理内存被占用了，该位为0，表示对应的物理内存可以使用。



**位图：**

对于虚拟地址和物理内存，都是要使用位图来管理的。

```c
struct bitmap
{
    uint32_t btmp_bytes_len;
    /* 在遍历位图时,整体上以字节为单位,细节上是以位为单位,所以此处位图的指针必须是单字节 */
    uint8_t *bits;
};
```

- ```btmp_bytes_len```表示位图的长度（以**字节**为单位）
  - 位图是用一位来表示一块空间的使用情况
    - 计算机中的存储单位是字节，比如从某个地址中取出数据，最少是取出一个字节
  - 所以要想知道这个位图有多少个位，要```btmp_bytes_len``` * 8
- 由于位图的长度不固定，所以使用指针来表示。
  - 所以存放位图实体，还需要另外的空间来存放（然后用指针指向该空间）



### 二、 内存管理的基本实现

#### 1. 虚拟地址池管理

内存管理，首先是需要管理虚拟地址池，用来管理所有的虚拟地址，也是使用位图来表示某一块虚拟地址是否被占用

```c
/* 用于虚拟地址管理 */
struct virtual_addr
{
    /* 虚拟地址用到的位图结构，用于记录哪些虚拟地址被占用了。以页为单位。*/
    struct bitmap vaddr_bitmap;
    /* 管理的虚拟地址起始地址 */
    uint32_t vaddr_start;
};
```

- ```struct bitmap vaddr_bitmap```表示虚拟地址池的位图，位图中，每一位表示1页物理内存空间，1页物理空间可以表示的地址就是4K。所以从虚拟地址池中，**申请虚拟地址，一次就是申请4K，然后把位图中某一位置为1**。
- ```uint32_t vaddr_start```，表示该虚拟地址池中的虚拟地址起始处。

结构如图所示：

![](https://gitee.com/imcgr/image_blog/raw/master/20210614110044.png)

- 绿色的小格子表示位图的1位
- 位图中的1位，对应虚拟地址池的1页（4KB），<font color="red">虚拟地址池并不存在于物理空间中，只是逻辑上存在</font>。
- <font color="red"><b>所谓的分配虚拟地址，只是把位图中的那一位设置为1了</b></font>

#### 2. 物理内存池管理

对于物理内存池，也是相同的套路，同样使用位图进行管理，只不过物理内存池是真实存在的物理内存，可以分配的。

```c
/* 内存池结构,生成两个实例用于管理内核内存池和用户内存池 */
struct pool
{
    struct bitmap pool_bitmap; // 本内存池用到的位图结构,用于管理物理内存
    uint32_t phy_addr_start;   // 本内存池所管理物理内存的起始地址
    uint32_t pool_size;        // 本内存池字节容量
    struct lock lock;          // 申请内存时互斥
};
```

- ```struct bitmap pool_bitmap```
  - 跟虚拟地址池类似，同样使用位图管理，**1位表示1页物理内存是否被使用**。
- ```uint32_t phy_addr_start```
  - 跟虚拟地址池类似，根据位图中的某1位在位图中的 **偏移量**，再**加上物理内存起始地址**，才是**最终的物理地址**。
- ```uint32_t pool_size```
  - 物理内存池的大小（单位：字节）
  - 物理内存池大小```pool_size```和位图中的长度```btmp_bytes_len```是不一样的。
- ```struct lock lock```
  - 由于**所有的进程/线程共用同一个物理内存池**（所有用户进程共用用户物理内存池，内核线程共用内核物理内存池），因此是互斥的，需要**加锁**！

结构如图所示：

![](https://gitee.com/imcgr/image_blog/raw/master/20210614085419.png)



- 物理内存池是真实存在的物理内存
- **物理内存池的每一页是否可用，完全由位图决定**。

现在有了数据结构来管理虚拟地址池和物理内存池，现在就是需要对虚拟地址池和物理内存池进行增删改查了。



#### 3. 初始化物理内存池

物理内存池的结构再次放出来：

```c
/* 内存池结构,生成两个实例用于管理内核内存池和用户内存池 */
struct pool
{
    struct bitmap pool_bitmap; // 本内存池用到的位图结构,用于管理物理内存
    uint32_t phy_addr_start;   // 本内存池所管理物理内存的起始地址
    uint32_t pool_size;        // 本内存池字节容量
    struct lock lock;          // 申请内存时互斥
};
```

对照着下面看看，初始化时，对于pool结构体的每个成员，是如何初始化的！

初始化物理内存池，这里简单介绍下：

```c
struct pool kernel_pool, user_pool; // 生成内核内存池和用户内存池
/* 初始化内存池 */
static void mem_pool_init(uint32_t all_mem)
{
    
    /* 0. 根据计算机中剩余的物理内存，分成两部分，内核物理内存池和用户物理内存池 */
    uint32_t page_table_size = PG_SIZE * 256;       // 页表大小= 1页的页目录表+第0和第768个页目录项指向同一个页表+
                                                    // 第769~1022个页目录项共指向254个页表,共256个页框
    uint32_t used_mem = page_table_size + 0x100000; // 0x100000为低端1M内存，结果为0x200000
    uint32_t free_mem = all_mem - used_mem;
    uint16_t all_free_pages = free_mem / PG_SIZE; // 1页为4k,不管总内存是不是4k的倍数,
                                                  // 对于以页为单位的内存分配策略，不足1页的内存不用考虑了。
    uint16_t kernel_free_pages = all_free_pages / 2;
    uint16_t user_free_pages = all_free_pages - kernel_free_pages;

    uint32_t kbm_length = kernel_free_pages / 8; // Kernel BitMap的长度,位图中的一位表示一页,以字节为单位
    uint32_t ubm_length = user_free_pages / 8;   // User BitMap的长度.

    uint32_t kp_start = used_mem;                               // Kernel Pool start,内核内存池的起始地址
    uint32_t up_start = kp_start + kernel_free_pages * PG_SIZE; // User Pool start,用户内存池的起始地址
	
    /*************************** 1. 设置物理内存池的起始物理地址 phy_addr_start *************************/
    	// 1.1 内核物理内存池的起始地址
    kernel_pool.phy_addr_start = kp_start;
    	// 1.2 用户物理内存池的起始地址
    user_pool.phy_addr_start = up_start;

    /************************** 2. 设置物理内存池的尺寸 pool_size  *********************************/
    	// 2.1 内核物理内存池的大小
    kernel_pool.pool_size = kernel_free_pages * PG_SIZE;
    	// 2.2 用户物理内存池的大小
    user_pool.pool_size = user_free_pages * PG_SIZE;

    /************************** 3. 设置物理内存池的位图长度 pool_bitmap.btmp_bytes_len **************/
    	// 3.1 内核物理内存池的位图长度
    kernel_pool.pool_bitmap.btmp_bytes_len = kbm_length;
    	// 3.2 用户物理内存池的位图长度
    user_pool.pool_bitmap.btmp_bytes_len = ubm_length;

	/************************** 4. 设置物理内存池的位图空间 pool_bitmap.bits  **********************/
    	// 4.1 设置内核物理内存池的位图空间
        // 内核使用的最高地址是0xc009f000,这是主线程的栈地址.(内核的大小预计为70K左右)
        // 32M内存占用的位图是2k.内核内存池的位图先定在MEM_BITMAP_BASE(0xc009a000)处.
    kernel_pool.pool_bitmap.bits = (void *)MEM_BITMAP_BASE; // MEM_BITMAP_BASE = 0xc009a000(虚拟地址)
    														// 0xc009a000会指向物理地址的0x9a000（因为页表是这样映射的）

    	// 4.2 设置用户物理内存池的位图空间
    	/* 用户内存池的位图紧跟在内核内存池位图之后 */
    user_pool.pool_bitmap.bits = (void *)(MEM_BITMAP_BASE + kbm_length);
    
    	// 4.3 把位图的空间置0
    bitmap_init(&kernel_pool.pool_bitmap);
    bitmap_init(&user_pool.pool_bitmap);
		
    	// 4.4 初始化锁
    lock_init(&kernel_pool.lock);
    lock_init(&user_pool.lock);
}
```

- 初始化物理内存池，要根据实际情况来。所以**物理内存池的起始地址**，不是从0开始。
  - 物理内存的低端1M被用了，然后0x1000 0 - 0x2000 0的物理内存处，放了256个内核页表，也被用了
  - 所以只剩下0x2000 0以上的地址可以分配
  - 所以**内核物理内存池**kernel_pool 的物理起始内存为kp_start (0x20000)
  - **用户物理内存池** 放在 内核物理内存池的上面，内核物理内存池 和 用户物理内存池管理的物理内存对半分
- 内核物理内存池和用户物理内存池的**大小**pool_size，就是从剩下的可用物理内存空间（0x20000 - 0xffff ffff）对半分
- 物理内存池的位图，固定放在虚拟地址```0xc009a000```处(对应物理地址```0x9a00```)
  - 从```0x9a00```到```9f000```这些内存空间，存放物理地址池的位图
  - 内核物理地址池位图和用户物理地址池位图，对半分
  - 从```0x9a00```到```9f000```这些内存空间，处于低端1MB中（**不在物理内存池起始地址范围内**），没有**算入可分配空间中**



#### 3. 初始化虚拟地址池

虚拟地址池，

- 对于内核线程，所有内核线程是共用同一个内核虚拟地址池
- 对于**用户进程，每一个用户进程有一个虚拟地址池**，所以用户进程的虚拟地址池，是在创建用户进程时初始化



**虚拟地址池的位图，也是要占用空间的，所以也是基于物理内存的，也要向物理内存池申请物理空间。**



虚拟地址池的结构，我也再次放出来，把虚拟地址池的结构体成员对照了初始化过程，看看每个成员如何初始化的！

```c
/* 用于虚拟地址管理 */
struct virtual_addr
{
    /* 虚拟地址用到的位图结构，用于记录哪些虚拟地址被占用了。以页为单位。*/
    struct bitmap vaddr_bitmap;
    /* 管理的虚拟地址起始地址 */
    uint32_t vaddr_start;
};
```



##### 3.1 初始化内核虚拟地址

```c
/* 初始化内核的虚拟地址 */
static void kernel_vaddr_init() 
{
    /************************** 5. 设置内核虚拟地址池 kernel_vaddr ***************************/
    	// 5.1 设置内核虚拟地址池 的 起始虚拟地址 vaddr_start，高端虚拟内存1G是内核空间
    kernel_vaddr.vaddr_start = K_HEAP_START; // K_HEAP_START = 0xc0100000（虚拟地址） 
    
    	// 5.2 设置内核虚拟地址池的长度  vaddr_bitmap.btmp_bytes_len
        /* 下面初始化内核虚拟地址的位图,按实际物理内存大小生成数组。*/
    kernel_vaddr.vaddr_bitmap.btmp_bytes_len = kbm_length; // 用于维护内核堆的虚拟地址,所以要和内核内存池大小一致
        
    	// 5.3 设置内核虚拟地址池的 位图空间 vaddr_bitmap.bits
        /* 位图的数组指向一块未使用的内存,目前定位在内核内存池和用户内存池之外*/
    kernel_vaddr.vaddr_bitmap.bits = (void *)(MEM_BITMAP_BASE + kbm_length + ubm_length);
		
    	// 5.4 初始化内核虚拟地址池的位图空间 -> 置为0
    bitmap_init(&kernel_vaddr.vaddr_bitmap);
}
```

> 注意：本函数是我上面的```mem_pool_init```函数分离出来的，本来这部分虚拟地址的初始化是和物理内存初始化一起的。

- 对于所有线程而言，把高端1G的虚拟地址映射到内核空间，低端3G的虚拟地址为自己进程空间。
  - 内核线程也是把高端1G的虚拟地址映射到内核空间
  - 由于内核线程本身就在内核中，也就没有自己的用户空间了
    - 所以操作系统不需要低端3G虚拟地址。即[0x0000 0000 - 0xc000 0000)
  - 因此操作系统的内核页目录表，只有后面的254项[769-1024]有值。
  - 也就是内核线程，的虚拟地址只有1G不到（从0xc0100000起）
- 内核虚拟地址的起始地址是K_HEAP_START = 0xc0100000
  - 0xc0100000是虚拟地址，要经过内核页表映射
    - 0xc010 0000 = <font color="red">1100 0000 00</font><font color="dodgerblue">10 0000 00000</font> 0000 0000 0000 
      - 1100 0000 00 = (768)<sub>10</sub>，也就是访问内核页目录表的第768项，该页目录表指向第1个内核页表（0号内核页表）
      - 10 0000 00000 = (256)<sub>10</sub>，也就是访问0号页表的第256项。
        - 当初在填充页表时，0号页表的[0-255]项，指向了低端1M物理内存。	所以从256项开始，就是可用的内存了。
  - 综上，0xc0100000虚拟地址，是最低的可用的虚拟地址。
- 疑问：那么内核页目录表和内核页表占用的地址怎么算？虚拟地址不算了吗？
  - 应该是，只要那一块物理内存没有使用页表映射到虚拟地址上，就可以不算了，因为使用虚拟地址根本访问不到。

##### 3.2 初始化用户虚拟池

虽然还没讲到用户进程，这里把用户进程创建虚拟地址位图的代码放出来，跟内核虚拟地址池对比下：

```c
/* 创建用户进程虚拟地址位图 */
void create_user_vaddr_bitmap(struct task_struct *user_prog)
{
    // 1. 设置用户进程的虚拟地址池 起始地址 vaddr_start
    user_prog->userprog_vaddr.vaddr_start = USER_VADDR_START; // USER_VADDR_START = 0x8048000
    uint32_t bitmap_pg_cnt = DIV_ROUND_UP((0xc0000000 - USER_VADDR_START) / PG_SIZE / 8, PG_SIZE);
    // 2. 设置用户进程虚拟地址池的位图
    user_prog->userprog_vaddr.vaddr_bitmap.bits = get_kernel_pages(bitmap_pg_cnt);
    user_prog->userprog_vaddr.vaddr_bitmap.btmp_bytes_len = (0xc0000000 - USER_VADDR_START) / PG_SIZE / 8;
    bitmap_init(&user_prog->userprog_vaddr.vaddr_bitmap);
}
```

- 每一个用户进程都有单独的虚拟地址池，因此用户进程的虚拟地址池放在PCB中
- 用户进程的虚拟地址 的 起始虚拟地址 为0x8048000
  - 至于为什么是0x8048000？
    - 听说Linux的32位可执行程序入口都是0x8048000，我也不知道为啥这样设计
- 对于用户进程的虚拟地址，可以看到范围是[0x8048000 - 0xc000 0000)
  - [0xc000 0000 - 0xffff ffff]这是高端1G，用于与内核交互。

#### 4. 分配4KB虚拟地址

一个程序，既需要物理地址，也需要虚拟地址。所以分配内存，其实是分配虚拟地址，然后分配物理内存，然后建立联系，这样才算分配完成。

从虚拟地址池中分配虚拟地址出来：

```c
/* 在pf表示的虚拟内存池中申请pg_cnt个虚拟页,
 * 成功则返回虚拟页的起始地址, 失败则返回NULL */
/*
	Description:
		在pf表示的虚拟内存池中申请pg_cnt个虚拟页
	Parameters:
		pf: enum pool_flags。表示内核虚拟地址池还是用户虚拟地址池
		pg_cnt: uint32_t。要申请的页数
	Returns:
		返回申请到的虚拟页起始地址
		return null if failed
	Detail:
		把虚拟地址池的位图中的位，修改为1
*/
static void *vaddr_get(enum pool_flags pf, uint32_t pg_cnt)
{
    int vaddr_start = 0, bit_idx_start = -1;
    uint32_t cnt = 0;
    if (pf == PF_KERNEL)
    {
        // 查找 虚拟地址池 的位图，是否有连续的pg_cnt位值为0（值为0表示该虚拟地址可用）
        bit_idx_start = bitmap_scan(&kernel_vaddr.vaddr_bitmap, pg_cnt);
        if (bit_idx_start == -1)
        {
            return NULL;
        }
        while (cnt < pg_cnt)
        {
            bitmap_set(&kernel_vaddr.vaddr_bitmap, bit_idx_start + cnt++, 1);
        }
        vaddr_start = kernel_vaddr.vaddr_start + bit_idx_start * PG_SIZE;
    }
    else
    { // 用户内存池
        struct task_struct *cur = running_thread();
        bit_idx_start = bitmap_scan(&cur->userprog_vaddr.vaddr_bitmap, pg_cnt);
        if (bit_idx_start == -1)
        {
            return NULL;
        }

        while (cnt < pg_cnt)
        {
            bitmap_set(&cur->userprog_vaddr.vaddr_bitmap, bit_idx_start + cnt++, 1);
        }
        vaddr_start = cur->userprog_vaddr.vaddr_start + bit_idx_start * PG_SIZE;

        /* (0xc0000000 - PG_SIZE)做为用户3级栈已经在start_process被分配 */
        ASSERT((uint32_t)vaddr_start < (0xc0000000 - PG_SIZE));
    }
    return (void *)vaddr_start;
}
```

- 分配虚拟地址比较简单。上面说过，虚拟地址池是逻辑上的，没有对应实体。只要**把虚拟地址池的位图中特定 位 设置为1**即可
- 所以，所谓的分配虚拟地址，步骤如下：
  - 在虚拟地址池的位图中，扫描pg_cnt个值为0的位（必须连续）
  - 然后把这几位设置为1
  - 完毕



#### 5. 分配4KB物理内存

分配的虚拟地址，最终要映射到物理地址中去，所以还需要分配物理内存。物理内存使用位图来描述每一页的使用情况，再次看看下面物理内存池的结构：

![](https://gitee.com/imcgr/image_blog/raw/master/20210614085419.png)

- <font color="red">物理内存池中真实的物理内存，是否可用，完全由位图决定</font>

所以，要管理物理内存池，只需要管理位图即可，而不用管物理内存中的内容。

所以所谓的分配物理内存，就是把位图中的一位设置为1即可。

```c
/*
    Description: 
        在m_pool(kernel_pool 或者 user_pool)指向的物理内存池中分配1个物理页
    Parameters:
        m_pool: struct pool*, the target memery pool
    Return:
        return physical address of page allcated if succeed
        return NULL if failed
*/
static void *palloc(struct pool *m_pool)
{
    /* 扫描或设置位图要保证原子操作 */
    int bit_idx = bitmap_scan(&m_pool->pool_bitmap, 1); // 找一个物理页面
    if (bit_idx == -1)
    {
        return NULL;
    }
    bitmap_set(&m_pool->pool_bitmap, bit_idx, 1); // 将此位bit_idx置1
    uint32_t page_phyaddr = ((bit_idx * PG_SIZE) + m_pool->phy_addr_start);
    return (void *)page_phyaddr;
}
```

- 这里分配物理内存跟分配虚拟地址类似，就是把位图中某一位设置为1

- 分配的物理内存粒度是1个物理页（4KB）



#### 6. 建立虚拟地址和物理内存的连接（设置页表）

分配内存，是分配虚拟地址和物理内存，并且要让虚拟的地址和物理内存(物理地址)的连接建立起来。

虚拟地址和物理内存的连接，是通过页表来实现的，所以要构建虚拟地址和物理地址的连接，也就是要**填充页表项**。

构建页表，是由操作系统来完成的。

##### 6.1 页表的特点

写代码之前，说一说页表的特点：

- 页目录表项，存放的是页表的**物理地址**
- 页表项，存放的是物理页框的**物理地址**
- 页目录表项的第1023项(最后一项)存放的是页目录表自身的**物理地址**
- 虚拟地址的高10位，用来定位页目录表中的项
- 虚拟地址中间10位，用来定位页表中的项
- 虚拟地址低12位，用来定位物理页，中的具体地址

##### 6.2 填充页目录表和页表步骤

所以，要填充页目录表和页表项，如下步骤：

- 访问**页目录表项**，填入**页表的物理地址**
- 访问**页表项**，填入**物理页的物理地址**



##### 6.3 如何填充页表和页目录表

我们已知：

- 物理页的物理地址（在分配物理内存时，我们获取到了物理页的物理地址）
- 虚拟地址

所以我们还需要：

- 访问，该虚拟地址 **将会**映射到的**页目录表项**（虚拟地址高10位就是页目录项）
- 访问，该虚拟地址 **将会**映射到的**页表项**（虚拟地址中间20位就是页表项）
- 页表的物理地址

##### 6.4 访问页目录表

<font color="dodgerblue">如何访问该**虚拟地址** **将会** 映射到的**页目录表项**和**页表项**呢？</font>

<font color="red">我们需要**构建一个虚拟地址**，该虚拟访问到该**目标虚拟地址**将会映射到的**页目录表项**</font>

这就使用到了页目录表的特性：

- 页目录表项的第1023项(最后一项)存放的是页目录表自身的**物理地址**

Talk is cheap, show me the code.

```c
/*
    Description:
        构建一个虚拟地址，可以访问vaddr必经的页目录项
    Detail:
        一个虚拟地址的高10位，是页目录表项的下标，可以得到页目录表项的相对地址偏移量。
        把构建一个地址0xffff f000  -> 高10位是1,中间10位也是1，可以访问到页目录表自身
        然后在这个地址的基础上，加上页目录表的地址相对偏移量，就得到了页目录表 项 

    Parameters:
        vaddr: 要构建页目录表项的虚拟地址
    Return:
        pde: 可以访问到 vaddr对应的页目录表项 的虚拟地址
            (也就是通过pde可以访问到vaddr必经的页目录表项)
*/
uint32_t *pde_ptr(uint32_t vaddr)
{
    /* 0xfffff是用来访问到页表本身所在的地址 */
    uint32_t *pde = (uint32_t *)((0xfffff000) + PDE_IDX(vaddr) * 4);// (vaddr & 0xffc00000) >> 22)
    return pde;
}
```

- 上面函数返回的pde，就是我们**构建好的虚拟地址**，该虚拟地址可以访问到vaddr必经的页表项
- 这里，取出vaddr对应的页目录表项 的 相对偏移地址，使用一个宏```#define PDE_IDX(vaddr) ((vaddr & 0xffc00000) >> 22)``` 
  - 就是取出vaddr的**高10位**

##### 6.5 访问页表

那么可以使用同样的手法，访问页表了。

```c
/*
    Description:
        构建一个虚拟地址，可以访问vaddr必经的页表项。
    Detail:
        先要设法访问页表项自身，然后加上地址的偏移量
    Parameters:
        vaddr: 要得到该地址页表项的虚拟地址
    Return:
*/
uint32_t *pte_ptr(uint32_t vaddr)
{
    /*
        PTE_IDX(vaddr) = ((vaddr & 0x003ff000) >> 12)
                取出vaddr的中间10位，得到vaddr必经的页表项的地址偏移量。
        
        (vaddr & 0xffc00000)取出虚拟地址高10位，这是vaddr将要映射的页目录项的下标 
        0xffc00000地址高10位是1, 让第一轮访问到的是页目录表自身
        0xffc00000 + ((vaddr & 0xffc00000)>>10) 让第二轮访问到的是页目录项，
        (0xffc00000 + ((vaddr & 0xffc00000) >> 10) + PTE_IDX(vaddr) * 4)
            让第三轮访问到页表项
   */
    uint32_t *pte = (uint32_t *)(0xffc00000 + ((vaddr & 0xffc00000) >> 10) + PTE_IDX(vaddr) * 4);
    return pte;
}
```

- 代码会有点绕，关键是要<font color="red">构建一个**虚拟地址**</font>，可以访问到vaddr 将会 映射到的 页表项

- 很绕的根本原因，就是<font color="red">开启分页后，所有给出的地址都是虚拟地址，即使我们知道物理地址，也要想方设法构建虚拟地址</font>



##### 6.6 填充页目录表和页表

现在可以访问目标地址vaddr 将会 经过的 页目录表和页表了，就来**填充页目录表项和页表项**了。

```c
/*
    Description:
        建立虚拟地址和物理地址的连接，即填充页表和页目录表
    Parameters:
        _vaddr: void*, 虚拟地址
        _page_phyaddr: void* 某一页的物理地址
*/
static void page_table_add(void *_vaddr, void *_page_phyaddr)
{
    uint32_t vaddr = (uint32_t)_vaddr, page_phyaddr = (uint32_t)_page_phyaddr;
    
    // 构建出了两个虚拟地址，pde和pte，这两个虚拟地址可以访问到vaddr必经的 页目录表项 和 页表项
    uint32_t *pde = pde_ptr(vaddr);
    uint32_t *pte = pte_ptr(vaddr);

    /* 先在页目录内判断目录项的P位，若为1,则表示该表已存在 */
    if (*pde & 0x00000001)
    {
        // 检测页目录表的P位
        if (!(*pte & 0x00000001))
        {                                                       // 只要是创建页表,pte就应该不存在,多判断一下放心
            *pte = (page_phyaddr | PG_US_U | PG_RW_W | PG_P_1); // US=1,RW=1,P=1
        }
        else
        { // 调试模式下不会执行到此,上面的ASSERT会先执行.关闭调试时下面的PANIC会起作用
            PANIC("pte repeat");
        }
    }
    else
    { // 页目录项不存在,所以要先创建页目录项再创建页表项.
        /* 页表中用到的页框一律从内核空间分配 */
        uint32_t pde_phyaddr = (uint32_t)palloc(&kernel_pool);
        *pde = (pde_phyaddr | PG_US_U | PG_RW_W | PG_P_1);

        /*******************   必须将页表所在的页清0   *********************
        * 必须把分配到的物理页地址pde_phyaddr对应的物理内存清0,
        * 避免里面的陈旧数据变成了页表中的页表项,从而让页表混乱.
        * pte的高20位会映射到pde所指向的页表的物理起始地址.
        * */
        memset((void *)((int)pte & 0xfffff000), 0, PG_SIZE);
        /************************************************************/
        ASSERT(!(*pte & 0x00000001));
        *pte = (page_phyaddr | PG_US_U | PG_RW_W | PG_P_1); // US=1,RW=1,P=1
    }
}
```



#### 7. 分配N页内存空间

上面实现了：

- 申请虚拟地址
- 申请物理内存
- 建立虚拟地址和物理内存的连接

上述三个步骤，就是申请内存的步骤了，把它们封装到一起，成一个函数：分配N页内存空间：

```c
/*
    Description:
        分配page_cnt页空间
    Detail:
        包括三个步骤：分配虚拟地址、分配物理内存地址、建立虚拟地址和物理地址连接
    Parameters:
        pf: enum pool_flags 物理内存池标志，标识着是内核物理内存池还是用户物理内存池
        pg_cnt: uint32_t 要申请的页数
    Return:
        vaddr_start: void* 申请成功后的虚拟地址
        return NULL if failed
*/
void *malloc_page(enum pool_flags pf, uint32_t pg_cnt)
{
    ASSERT(pg_cnt > 0 && pg_cnt < 3840);
    /***********   malloc_page的原理是三个动作的合成:   ***********
      1通过vaddr_get在虚拟内存池中申请虚拟地址
      2通过palloc在物理内存池中申请物理页
      3通过page_table_add将以上两步得到的虚拟地址和物理地址在页表中完成映射
    ***********************************************************/
    void *vaddr_start = vaddr_get(pf, pg_cnt);
    if (vaddr_start == NULL)
    {
        return NULL;
    }
    uint32_t vaddr = (uint32_t)vaddr_start, cnt = pg_cnt;
    struct pool *mem_pool = pf & PF_KERNEL ? &kernel_pool : &user_pool;

    /* 因为虚拟地址是连续的,但物理地址可以是不连续的,所以逐个做映射*/
    while (cnt-- > 0)
    {
        void *page_phyaddr = palloc(mem_pool);

        // 失败时要将曾经已申请的虚拟地址和物理页全部回滚，在将来完成内存回收时再补充
        if (page_phyaddr == NULL)
        {
            return NULL;
        }
        // 填充页目录表项和页表项
        page_table_add((void *)vaddr, page_phyaddr); 
        vaddr += PG_SIZE;                            
    }
    return vaddr_start;
}
```



#### 8. 分配N页内核内存空间

上面已经实现了 分配N页内存空间，现在指定内核物理内存池再次封装一下，改成分配N页内核空间

```c
/*
    Description:
        从内核内存空间中分配pg_cnt页(包括虚拟地址和物理内存以及之间的页表映射)
    Parameters:
        pg_cnt: uint32_t 要申请的页数
    Return:
        vaddr: 成功申请空间后，返回的虚拟地址
        return NULL if failed
*/
void *get_kernel_pages(uint32_t pg_cnt)
{
    lock_acquire(&kernel_pool.lock);
    void *vaddr = malloc_page(PF_KERNEL, pg_cnt);
    if (vaddr != NULL)
    { // 若分配的地址不为空,将页框清0后返回
        memset(vaddr, 0, pg_cnt * PG_SIZE);
    }
    lock_release(&kernel_pool.lock);
    return vaddr;
}
```

- 这里是对分配N页空间```malloc_page```的封装，指定了从内核内存分配
  - 分配同样包含三个步骤：
    - 申请虚拟地址
    - 申请物理内存空间
    - 建立映射（填充页表）
- 在分配内存空间时<font color="red">加锁</font>，防止线程安全问题



#### 9. 分配N页用户内存空间

分配N页用户内存空间，几乎和分配N页内核内存空间一样的，就是指定的物理内存池不一样

```c
/* 在用户空间中申请4k内存,并返回其虚拟地址 */
void *get_user_pages(uint32_t pg_cnt)
{
    lock_acquire(&user_pool.lock);
    void *vaddr = malloc_page(PF_USER, pg_cnt);
    memset(vaddr, 0, pg_cnt * PG_SIZE);
    lock_release(&user_pool.lock);
    return vaddr;
}
```

- 指定物理内存池是 用户物理内存池（PF_USER用来标识）

#### 10. 根据虚拟地址分配1页物理内存

这里跟上面的 9. 分配N页内存空间 类似.

只不过，这里是**已知了虚拟地址**。所以只需要从物理内存池中分配物理地址，然后构建页表，建立映射即可。

```c
/*
    Description: 
        已知虚拟地址vaddr，申请物理内存，并建立映射
    Details:
        已知了虚拟地址vaddr，所以剩下两个步骤：
            - 申请物理内存 palloc
            - 建立虚拟地址和物理内存的连接  page_table_add
    Parameters:
        pf: 内核内存池或者用户内存池
        vaddr:  已知的虚拟地址 given virtual address
    Return:
        vaddr: 和传递进来的参数一样
*/
void *get_a_page(enum pool_flags pf, uint32_t vaddr)
{
    struct pool *mem_pool = pf & PF_KERNEL ? &kernel_pool : &user_pool;
    lock_acquire(&mem_pool->lock);

    /* 先将虚拟地址对应的位图置 -1 */
    struct task_struct *cur = running_thread();
    int32_t bit_idx = -1;

    /* 若当前是用户进程申请用户内存,就修改用户进程自己的虚拟地址位图 */
    if (cur->pgdir != NULL && pf == PF_USER)
    {
        bit_idx = (vaddr - cur->userprog_vaddr.vaddr_start) / PG_SIZE;
        ASSERT(bit_idx > 0);
        bitmap_set(&cur->userprog_vaddr.vaddr_bitmap, bit_idx, 1);
    }
    else if (cur->pgdir == NULL && pf == PF_KERNEL)
    {
        /* 如果是内核线程申请内核内存,就修改kernel_vaddr. */
        bit_idx = (vaddr - kernel_vaddr.vaddr_start) / PG_SIZE;
        ASSERT(bit_idx > 0);
        bitmap_set(&kernel_vaddr.vaddr_bitmap, bit_idx, 1);
    }
    else
    {
        PANIC("get_a_page:not allow kernel alloc userspace or user alloc kernelspace by get_a_page");
    }

    void *page_phyaddr = palloc(mem_pool);
    if (page_phyaddr == NULL)
    {
        return NULL;
    }
    page_table_add((void *)vaddr, page_phyaddr);
    lock_release(&mem_pool->lock);
    return (void *)vaddr;
}
```

- 已知了**虚拟地址**，因此剩下两个步骤
  - 申请**物理内存** （```pmalloc```函数实现）
  - 建立虚拟地址和物理内存的连接（**构建页表**）（```page_table_add```函数实现）
- 主要记得**加锁**

### 三、内存管理进一步优化

上面的分配内存，都是以1个物理页为单位，也就是**最少分配4KB**的内存，粒度算是有点大的。

平时使用C语言的```malloc```函数，分配内存，都是以字节为单位的。如何让**分配内存的粒度更细**一点呢？

#### 1. arena

要想分配粒度更细的物理内存，可以在1个物理页的基础上，**分成更小的块**。

- 我们把1个物理页，分成更小的块，这些块就叫<font color="red"><b>```mem_block```</b></font>。
- <font color="red">注意，不是一开始就把所有的物理页，都分成了很多的mem_block，而是在分配内存时，需要的时候才去把物理页分成mem_block</font>
- 在1个物理页中有很多```mem_block```，就需要一种结构管理这些小的```mem_block```，那这就是<font color="red"><b>```arena```</b></font>。
  - ```arena```就像是一个仓库，里面有很多```mem_block```

##### 1.1 arena基本结构

arena基本结构如下图：

![](https://gitee.com/imcgr/image_blog/raw/master/20210615092603.png)

可以看出，arena有一下特性：

- 一个arena占用一个物理页（**4KB**）
- 一个arena（占1个物理页，4KB）分成了**两部分**：
  - **arena元信息**，用来描述这个arena
  - **mem_block**，可以**分配的物理内存块**
- 同一个arena中的mem_block大小都是一样的
- 由于arena占用了1个物理页，所以1个物理页中，真正可以分配的内存（所有的mem_block）不足4KB了。



看了上述结构，就有两个问题：

- arena元信息有什么内容？
- mem_block多大比较合适？

##### 1.2. mem_block规格

这里把```mem_block```分成7种不同的规格大小：16字节、32字节、64字节、128字节、256字节、512字节、1024字节。

同一个arena中放相同规格大小的```mem_block```，所以不同规格的arena，有着不同数量的```mem_block```。

如下图：

![](https://gitee.com/imcgr/image_blog/raw/master/20210615094544.png)





对于需要的内存块超过了1024字节，如何分配呢？

直接按页连续分配：

<img src="https://gitee.com/imcgr/image_blog/raw/master/20210615122451.png" style="zoom: 80%;" />

- 如上图，现在需要分配4KB内存
- 超过了1024字节，只分配1物理页有不够，只好分配2个物理页
- 这两个物理页是连在一起的，**这几个连续的物理页共用一个arena头部（arena元信息）**
- 所以此时arena大小是2个物理页。在arena的large字段为true，cnt为2



##### 1.3 内存块描述符 mem_block_desc

不同规格的```mem_block```都是散落的，如何集中管理呢？

这里把规格相同的内存块```mem_block```，串成链表，相同规格的内存块```mem_block```组成的链表，形成一个内存块描述符```mem_block_desc```。



内存块描述符```mem_block_desc```的结构如下所示：

```c
/* 内存块描述符 */
struct mem_block_desc {
   uint32_t block_size;			 // 内存块大小
   uint32_t blocks_per_arena;	 // 本arena中可容纳此mem_block的数量.
   struct list free_list;	 	 // 目前可用的mem_block链表
};
```

- ```block_size```：内存块规格
  - 一个内存块描述符 管理 内存块规格大小相同的内存块
- ```blocks_per_arena```：内存块中的arena有几个这样规格的内存块
- ```free_list```：
  - 所有可用的内存块

> <font color="dodgerblue">这个时候你可能会想，每一个```mem_block```都表示可以分配的内存，怎么串起来呢？给每个内存块```mem_block```添加一个链表节点吗？那么这个内存块的可用内存不是又减少了吗（因为一部分用于存储链表节点）？</font>
>
> - **对的，就是给每一个内存块```mem_block```都设置为1个链表节点，然后串起来**
> - **但是，每个```mem_block```可以分配的物理内存并没有减少**
>   - <font color="red">```mem_block```只有在串起来的时候，才需要存储 链表 节点这个数据结构，**如果把它分配出去了，链表节点这个结构就没用了**。</font>
>   - <font color="red">当一个```mem_block```分配出去后，原本链表节点占用的空间，会被初始化，一起作为可分配内存分配出去了</font>
>   - <font color="red">相当于只是**临时借用**了一下```mem_block```中的空间，用来保存链表节点的数据结构</font>
> - <b>有一说一，这种方式太妙了，把内存空间利用的淋漓尽致</b>



每个进程，都有有自己独立的内存块描述符表（里面是不同规格的内存块描述符），放在PCB中。



#### 2. arena内存管理基本结构

根据上面的知识，再次梳理下结构：

- 一个物理页，就是一个arena
- 一个arena分成多个内存块mem_block
- 相同规格的内存块，串在一起，由内存块描述符```mem_block_desc```管理

- 那么有7种规格的内存块，因此有7个内存块描述符```mem_block_desc```



**简化**的结构如下：

![](https://gitee.com/imcgr/image_blog/raw/master/20210615102522.png)

#### 3. arena、mem_block、mem_block_desc结构

##### 3.1 arena元信息结构

```c
/* 内存仓库arena元信息 */
// 字节对齐后，占用12字节
struct arena
{
    struct mem_block_desc *desc; // 此arena关联的mem_block_desc
    uint32_t cnt;
    bool large;					// large为true，cnt表示的是页框数
    							// large为false，cnt表示的是空闲mem_block数量
};
```

- 每一个仓库arena的内存块```mem_block```的规格都是一样的，所以一个仓库arena指向一个内存块描述符```mem_block_desc```
- 内存块规格只有7种，最大只有1024字节的，如果分配超过1024字节，就分配整个物理页框



##### 3.1 内存块mem_block结构

```c
/* 内存块 */
struct mem_block
{
    struct list_elem free_elem;
};
```

- mem_block是被串起来的，所以主要是保存链表节点的结构
- 注意：<font color="red">```mem_block```保存的链表节点结构，只有在没有分配出去（串在内存块描述符中）时，才有用。```mem_block```分配出去后，这个结构就没用了，一起作为可用内存分配出去</font>



##### 3.2 内存块描述符mem_block_desc结构

```c
/* 内存块描述符 */
struct mem_block_desc
{
    uint32_t block_size;       // 内存块大小
    uint32_t blocks_per_arena; // 本arena中可容纳此mem_block的数量.
    struct list free_list;     // 目前可用的mem_block链表
};
```

- 内存块描述符```mem_block_desc```用于把可用的内存块```mem_block```串起来
- 内存块描述符串相同规格的内存块



#### 4. 初始化内存块描述符

内存描述符有7种规格，这里是构建出7个内存块描述符出来：

```c
/*
    Description:
        构建出DESC_CNT种规格的内存块描述符
    Details:
        主要是设置mem_block_desc的几个成员
    Parameters:
        desc_array: 需要进行初始化的内存块描述符表（包含7个内存块描述符）
*/
void block_desc_init(struct mem_block_desc *desc_array)
{
    uint16_t desc_idx, block_size = 16;

    /* 初始化每个mem_block_desc描述符 */
    for (desc_idx = 0; desc_idx < DESC_CNT; desc_idx++)
    {
        desc_array[desc_idx].block_size = block_size;

        /* 初始化arena中的内存块数量 */
        desc_array[desc_idx].blocks_per_arena = (PG_SIZE - sizeof(struct arena)) / block_size;

        list_init(&desc_array[desc_idx].free_list);

        block_size *= 2; // 更新为下一个规格内存块
    }
}
```

#### 5. 分配内存块

分配内存块有一下几个步骤：

- 找到要分配的物理内存池（内核内存池还是用户内存池）
- 看要分配内存大小```size```，是否超过了1024字节
  - 如果超过了1024字节，就按页连续分配
    - 需要分配的页数：```uint32_t page_cnt = DIV_ROUND_UP(size + sizeof(struct arena), PG_SIZE);```
    - 也就是这连续的多个物理页，形成一个arena
  - 如果少于或者等于1024个字节，就找合适的规格的mem_block分配
    - 先到该任务的```mem_block_desc```表中找该规格的内存块链表，有没有多余的
    - 如果有多余的，就直接分配，也就是把内存块链表中的内存块弹出
    - 如果没有多余的，就申请1页，然后切成块，放到链表中



```c
/*
    Description:
        在堆中分配size字节的内存空间
    Parameters:
        size: 需要的内存空间大小
    Return:
        分配后的空间的虚拟地址
    Details:
        分配时，有两种情况：
            - 规格大于1024，那么分配内存按照1个物理页为单位分配
            - 规格小于1024，就分配内存块mem_block
        
        分配内存块mem_block时：
            - 先取出1个物理页
            - 然后选择合适的规格，切碎，分成小块mem_block
            - 把全部小块mem_block串起来成列表
            - 然后分一块mem_block出去
*/
void *sys_malloc(uint32_t size)
{
    enum pool_flags PF;
    struct pool *mem_pool;
    uint32_t pool_size;

    // 内存块描述符表（里面是不同规格的内存块描述符）
    struct mem_block_desc *descs;
    struct task_struct *cur_thread = running_thread();

    /***********1. 找出要操作的物理内存池************/

    if (cur_thread->pgdir == NULL)
    { // 若为内核线程
        PF = PF_KERNEL;
        pool_size = kernel_pool.pool_size;
        mem_pool = &kernel_pool;

        // 内核 内存块描述符表
        descs = k_block_descs;
    }
    else
    {
        // 用户进程pcb中的pgdir会在为其分配页表时创建
        PF = PF_USER;
        pool_size = user_pool.pool_size;
        mem_pool = &user_pool;

        // 用户进程独立的内存块描述符表
        descs = cur_thread->u_block_desc;
    }

    /* 若申请的内存不在内存池容量范围内则直接返回NULL */
    if (!(size > 0 && size < pool_size))
    {
        return NULL;
    }

    struct arena *a;
    struct mem_block *b;
    lock_acquire(&mem_pool->lock);

    /*****************2. 分配内存*************************/

    /********* 超过最大内存块1024, 就以页框为分配单位 ********/
    if (size > 1024)
    {
        // 需要的连续page_cnt物理页
        uint32_t page_cnt = DIV_ROUND_UP(size + sizeof(struct arena), PG_SIZE); // 向上取整需要的页框数

        // 那么1个arena就需要page_cnt页
        a = malloc_page(PF, page_cnt);

        if (a != NULL)
        {
            memset(a, 0, page_cnt * PG_SIZE); // 将分配的内存清0
            // 对于分配的大块页框,将desc置为NULL, cnt置为页框数,large置为true
            a->desc = NULL;
            a->cnt = page_cnt;
            a->large = true;
            lock_release(&mem_pool->lock);
            return (void *)(a + 1); // 跨过arena大小，把剩下的内存返回
        }
        else
        {
            lock_release(&mem_pool->lock);
            return NULL;
        }
    }
    /**************若申请的内存小于等于1024 ***********************/
    else
    {
        uint8_t desc_idx;
        // 从内存块描述符中匹配合适的内存块规格
        for (desc_idx = 0; desc_idx < DESC_CNT; desc_idx++)
        {
            if (size <= descs[desc_idx].block_size)
            {
                // 从小往大后,找到后退出
                break;
            }
        }

        // 在需要的规格中的内存块描述符中的列表中，看有没有可用的
        if (list_empty(&descs[desc_idx].free_list))
        {
            // 如果没有可用的，就申请1个物理页，然后切碎成mem_block
            a = malloc_page(PF, 1); // 分配1页框做为arena
            if (a == NULL)
            {
                lock_release(&mem_pool->lock);
                return NULL;
            }
            memset(a, 0, PG_SIZE);

            a->desc = &descs[desc_idx];
            a->large = false;
            a->cnt = descs[desc_idx].blocks_per_arena;
            uint32_t block_idx;

            enum intr_status old_status = intr_disable();

            // 开始将arena拆分成内存块,并添加到内存块描述符的free_list中
            for (block_idx = 0; block_idx < descs[desc_idx].blocks_per_arena; block_idx++)
            {
                b = arena2block(a, block_idx);
                ASSERT(!elem_find(&a->desc->free_list, &b->free_elem));
                list_append(&a->desc->free_list, &b->free_elem);
            }
            intr_set_status(old_status);
        }

        // 从可用的内存块链表中，取出一个内存块出来
        b = elem2entry(struct mem_block, free_elem, list_pop(&(descs[desc_idx].free_list)));
        memset(b, 0, descs[desc_idx].block_size);

        a = block2arena(b); // 获取内存块b所在的arena
        a->cnt--;           // 将此arena中的空闲内存块数减1
        lock_release(&mem_pool->lock);
        return (void *)b;
    }
}
```

关于上述代码，要注意的几点

- 分配```mem_block```是对分配物理页(```mallock_page```)的封装
- <font color="red">分配的时候，还是按页分配的，分配1个物理页过来，只不过没用完的空间保存了起来，下次备用。</font>
- 在分配内存大于1024时，为什么不直接使用```malloc_page```分配连续的页呢？而是仍然按照arena来分配，而且arena需要元信息，不是浪费空间吗？
  - 我想，是因为想弄一个统一的接口，来分配内存吧。这样不管需要的内存大小，都调用```sys_malloc```函数，都使用arena格式来分配。



上述代码，用到了两个函数：```arena2block```和```block2arena```：

```arena2block```函数：

```c
/*
    Description:
        返回arena中下标为idx的内存块的地址
    Parameters:
        a: struct arena*。要操作的arena
        idx: arena下标。目标内存块所在的下标
    Return:
        arena 中下标idx的内存块的地址
*/
static struct mem_block *arena2block(struct arena *a, uint32_t idx)
{
    return (struct mem_block *)((uint32_t)a + sizeof(struct arena) + idx * a->desc->block_size);
}
```

```block2arena```函数：

```c
/*
    Descrption:
        返回内存块b所在的arena地址
    Parameters:
        b: struct mem_block*。内存块的地址
    Return:
        内存块b所在的 arena的地址
    Details:
        直接高20位取整，因为内存块中1个arena就是1个物理页
*/
static struct arena *block2arena(struct mem_block *b)
{
    return (struct arena *)((uint32_t)b & 0xfffff000);
}
```



### 四、内存回收

内存申请了，就可以回收。内存回收就是内存申请的逆过程。

内存申请包括3个步骤：

- 申请虚拟地址
- 申请物理空间
- 建立虚拟地址和物理空间的联系（填充页表）

那么回收空间，也是这几个步骤：

- 回收物理空间
- 取消虚拟地址和物理空间的联系（把页表的P位设置为0即可）
  - 注意，页目录表和页表的内容没变，只是把页表的P位设置为了1（没设置页目录表的P位）
- 回收虚拟空间

#### 1. 回收物理内存

```c
/* 将物理地址pg_phy_addr回收到物理内存池 */
void pfree(uint32_t pg_phy_addr)
{
    struct pool *mem_pool;
    uint32_t bit_idx = 0;
    if (pg_phy_addr >= user_pool.phy_addr_start)
    { // 用户物理内存池
        mem_pool = &user_pool;
        bit_idx = (pg_phy_addr - user_pool.phy_addr_start) / PG_SIZE;
    }
    else
    { // 内核物理内存池
        mem_pool = &kernel_pool;
        bit_idx = (pg_phy_addr - kernel_pool.phy_addr_start) / PG_SIZE;
    }
    bitmap_set(&mem_pool->pool_bitmap, bit_idx, 0); // 将位图中该位清0
}
```

- 所谓回收物理内存，只要把某个地址相应的位图的某一位，设置为0即可

- 注意，这里位图的1位，表示了4KB物理内存空间的使用情况，所以这是一次性回收了4KB内存空间



#### 2. 取消虚拟地址和物理内存的映射

```c
/* 去掉页表中虚拟地址vaddr的映射,只去掉vaddr对应的pte的P位 */
static void page_table_pte_remove(uint32_t vaddr)
{
    uint32_t *pte = pte_ptr(vaddr);
    *pte &= ~PG_P_1; // 将页表项pte的P位置0
    asm volatile("invlpg %0" ::"m"(vaddr) : "memory"); //更新tlb
}
```

- 把该虚拟地址vaddr，对应的页表项的P位设置为0



#### 3. 回收虚拟地址

```c
/* 在虚拟地址池中释放以_vaddr起始的连续pg_cnt个虚拟页地址 */
static void vaddr_remove(enum pool_flags pf, void *_vaddr, uint32_t pg_cnt)
{
    uint32_t bit_idx_start = 0, vaddr = (uint32_t)_vaddr, cnt = 0;

    if (pf == PF_KERNEL)
    { // 内核虚拟内存池
        bit_idx_start = (vaddr - kernel_vaddr.vaddr_start) / PG_SIZE;
        while (cnt < pg_cnt)
        {
            bitmap_set(&kernel_vaddr.vaddr_bitmap, bit_idx_start + cnt++, 0);
        }
    }
    else
    { // 用户虚拟内存池
        struct task_struct *cur_thread = running_thread();
        bit_idx_start = (vaddr - cur_thread->userprog_vaddr.vaddr_start) / PG_SIZE;
        while (cnt < pg_cnt)
        {
            bitmap_set(&cur_thread->userprog_vaddr.vaddr_bitmap, bit_idx_start + cnt++, 0);
        }
    }
}
```

- 虚拟地址，同样是在虚拟地址池中，将某个地址相应的位，设置为0



#### 4. 按页回收内存空间（上面三点封装）

- 回收物理内存空间（位图相应位设置为0）
- 取消虚拟地址和物理地址联系（页表项的P位设置为0）
- 回收虚拟地址（位图对应位设置为0）

```c
/*
    Description:
        释放以虚拟地址vaddr为起始的cnt个物理页框
    Parameters:
        pf: enum pool_flags 表明了用户内存池还是内核内存池
        _vaddr: void*。 要释放的虚拟地址
        pg_cnt: uint32_t。要释放的连续pg_cnt页
    Details:

*/
void mfree_page(enum pool_flags pf, void *_vaddr, uint32_t pg_cnt)
{
    uint32_t pg_phy_addr;
    uint32_t vaddr = (int32_t)_vaddr, page_cnt = 0;
    ASSERT(pg_cnt >= 1 && vaddr % PG_SIZE == 0);
    pg_phy_addr = addr_v2p(vaddr); // 获取虚拟地址vaddr对应的物理地址

    /* 确保待释放的物理内存在低端1M+1k大小的页目录+1k大小的页表地址范围外 */
    ASSERT((pg_phy_addr % PG_SIZE) == 0 && pg_phy_addr >= 0x102000);

    /* 判断pg_phy_addr属于用户物理内存池还是内核物理内存池 */
    if (pg_phy_addr >= user_pool.phy_addr_start)
    {
        // 位于user_pool内存池
        vaddr -= PG_SIZE;
        while (page_cnt < pg_cnt)
        {
            vaddr += PG_SIZE;
            pg_phy_addr = addr_v2p(vaddr);

            // 释放物理地址
            pfree(pg_phy_addr);

            // 取消虚拟地址和物理地址的联系 
            // 清除虚拟地址对应的页表项pte的P位 */
            page_table_pte_remove(vaddr);
            page_cnt++;
        }

        // 释放虚拟地址
        vaddr_remove(pf, _vaddr, pg_cnt);
    }
    else
    { 
        // 位于kernel_pool内存池
        vaddr -= PG_SIZE;
        while (page_cnt < pg_cnt)
        {
            vaddr += PG_SIZE;
            pg_phy_addr = addr_v2p(vaddr);

            // 释放物理地址
            pfree(pg_phy_addr);

            // 清除页表项的P位
            page_table_pte_remove(vaddr);

            page_cnt++;
        }
        // 释放虚拟地址
        vaddr_remove(pf, _vaddr, pg_cnt);
    }
}
```



#### 5. 回收内存仓库arena

上面4点，以1页单位，回收内存空间。

但是在之前申请内存时，我们做了一个封装，把申请内存的都封装成了arena的形式：

- 申请的字节大小大于1024时
  - 以物理页为单位，连续分配多页。这多个页为组成1个arena
- 申请的字节小于1024时
  - 把1个物理页作为1个arena，然后分成多个```mem_block```
  - 分配空间以```mem_block```为单位，分配1个```mem_block```



因此这里回收空间时，对按页分配做一层封装，让分配的粒度更细。



分配流程如下：

- 要回收的地址，就是**mem_block的起始地址**
- 根据mem_block的地址，取高20位为有效位，得到**arena的地址**
- 然后根据得到的arena，的large属性和cnt属性，可以看出当时是**大块分配(按页)**，**小块分配(按mem_block)**
  - 如果是大块分配，就直接释放cnt个物理页
  - 如果是小块分配，就要释放时，先**把mem_block放回空闲列表**
    - 在放回去后，判断下该mem_block所在的arena中的内存块是不是都是空闲的
      - 如果都是空闲的，那么这1页就可以释放了



```c
/*
    Description:
        回收地址为ptr的内存空间
    Parameters:
        ptr: void* 要回收的虚拟地址
    Details:
        要释放的地址ptr, 也就是当时申请空间时的mem_block起始地址
        根据mem_block的起始地址，低12位取0，就得到该mem_block对应的arena地址

        然后根据arena的large和cnt属性，可以看出当时是大块分配(按页)，小块分配(按mem_block)
            - 如果是大块分配，就直接大块释放cnt个页
            - 如果是小块分配，就要释放时，先把mem_block放回空闲列表
                - 在放回去后，判断下该mem_block所在的arena中的内存块空闲个数
                    如果都是空闲的，那么这1页就可以释放了
*/
void sys_free(void *ptr)
{
    ASSERT(ptr != NULL);
    if (ptr != NULL)
    {
        enum pool_flags PF;
        struct pool *mem_pool;

        // 如果当前是内核线程
        if (running_thread()->pgdir == NULL)
        {
            ASSERT((uint32_t)ptr >= K_HEAP_START);
            PF = PF_KERNEL;
            mem_pool = &kernel_pool;
        }
        // 如果当前是用户进程
        else
        {
            PF = PF_USER;
            mem_pool = &user_pool;
        }

        lock_acquire(&mem_pool->lock);

        struct mem_block *b = ptr;

        // 把mem_block转换成arena,获取元信息
        struct arena *a = block2arena(b);
        ASSERT(a->large == 0 || a->large == 1);
        if (a->desc == NULL && a->large == true)
        {
            // 大于1024的内存，直接把该页释放
            mfree_page(PF, a, a->cnt);
        }
        else
        {
            // 小于等于1024的内存块，先将内存块回收到free_list
            list_append(&a->desc->free_list, &b->free_elem);

            // 再判断此arena中的内存块是否都是空闲,如果是就释放arena
            if (++a->cnt == a->desc->blocks_per_arena)
            {
                uint32_t block_idx;
                for (block_idx = 0; block_idx < a->desc->blocks_per_arena; block_idx++)
                {
                    struct mem_block *b = arena2block(a, block_idx);
                    ASSERT(elem_find(&a->desc->free_list, &b->free_elem));
                    list_remove(&b->free_elem);
                }
                mfree_page(PF, a, 1);
            }
        }
        lock_release(&mem_pool->lock);
    }
}
```

- 终于知道了C语言中的free是怎么确定释放内存大小的
  - 在分配内存时，是按照一定规格的，所以释放时，也按照这个规格来释放
- 总于知道了，```arena```中的large和cnt的价值，还有```mem_block_desc```中的```blocks_per_arena```属性的价值
  - ```arena```的large属性和cnt属性，在释放空间时才有用
    - 根据large属性，决定是不是可以按页直接释放
      - 如果是按页释放，根据cnt属性，决定释放多少页
  - ```mem_block_desc```中的```blocks_per_arena```属性，在释放时，和arena的```cnt```属性判断
    - 如果```block_per_arena```和```cnt```相等，说明arena中空闲块数量 和 该arena的最大块数量相等，也就是所有块都是空闲的
      - 那么这一个arena（1个物理页）就可以释放了



### 五、 代码调用图

#### 1. 内存管理初始化：

- 初始化物理内存池（包括内核物理内存池和用户物理内存池）
- 初始化内核虚拟地址池（用户进程的虚拟地址池在创建用户进程时初始化）
- 初始化 内存块规格描述符表（初始化多种规格的内存块描述符）

下图是内存初始化过程的代码调用（图片较大，无法预览？<a href="https://gitee.com/imcgr/image_blog/raw/master/20210615164315.png">图片</a>）：

![](https://gitee.com/imcgr/image_blog/raw/master/20210615164315.png)



#### 2. 申请内存

- 申请虚拟地址
- 申请物理内存
- 建立虚拟地址和物理内存的联系（填页表）

用代码实现，调用关系如下图：

如下图：似乎太大了，无法预览？<a href="https://gitee.com/imcgr/image_blog/raw/master/20210615171848.png">查看图片</a>

![](https://gitee.com/imcgr/image_blog/raw/master/20210615171848.png)



#### 3. 释放内存

- 释放物理地址
- 取消虚拟地址和物理地址的联系（设置页表的P位）
- 释放虚拟地址

如下图（图片太大，无法预览？<a href="https://gitee.com/imcgr/image_blog/raw/master/20210615185051.png">图片</a>）：

![](https://gitee.com/imcgr/image_blog/raw/master/20210615185051.png)

